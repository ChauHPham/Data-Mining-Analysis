{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfc73a1",
   "metadata": {},
   "source": [
    "# **DBScan Clustering - CMPT 459 Course Project**\n",
    "\n",
    "This notebook performs **Density-based clustering** on the diabetic patient dataset.\n",
    "\n",
    "We do the following:\n",
    "- Data preprocessing consistent with the project pipeline\n",
    "- PCA for dimensionality reduction for visualization of results\n",
    "- 2D/3D PCA visualization of best clustering \n",
    "- Custom implemention of **density-based clustering** ( `dbscan_clustering.py`)\n",
    "- Silhouette score as a metric to evaluate cluster quality\n",
    "\n",
    "This notebook is part of our group’s modular report and references:\n",
    "- `dbscan_clustering.py`\n",
    "- `dbscan_clustering_analysis.py` (original script version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bffcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from dbscan_clustering import DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee3abd",
   "metadata": {},
   "source": [
    "##  Loading and Preprocessing Data \n",
    "\n",
    "Following the same loading and preprocessing data process as the rest of the project, we apply the following:\n",
    "\n",
    "- Replace `'?'` values with `NaN`  \n",
    "- Drop columns with >40% missing values \n",
    "- One-hot encode high-cardinality categorical columns  \n",
    "- Label-encode low-cardinality categorical features  \n",
    "- Normalize numerical features  \n",
    "- Encode our target variable `readmitted` as integers:  \n",
    "  - `NO → 0`, `>30 → 1`, `<30 → 2`  \n",
    "- Remove sensitive/identifying data: `encounter_id`, `patient_nbr`\n",
    "\n",
    "Finally, we sample **1,000** rows (default) to allow DBScan to run in a reasonable amount of time as the algorithm runs in worst case **O(n^2)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "    # Replace '?'\n",
    "    df = df.replace('?', np.nan)\n",
    "\n",
    "    # Drop >40% missing\n",
    "    threshold = 0.4 * len(df)\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "    # Fill remaining categorical NA\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "    # Encode target\n",
    "    df[\"readmitted\"] = df[\"readmitted\"].map({'NO':0, '>30':1, '<30':2})\n",
    "\n",
    "    # Encode categorical\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    le = LabelEncoder()\n",
    "    for col in cat_cols:\n",
    "        if df[col].nunique() < 10:\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "        else:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "    # Remove ID columns\n",
    "    for col in [\"encounter_id\", \"patient_nbr\"]:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "\n",
    "    # Normalize numeric\n",
    "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    scaler = StandardScaler()\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "    print(\"Preprocessing complete!\")\n",
    "    print(\"Final shape:\", df.shape)\n",
    "\n",
    "    target = df[\"readmitted\"].copy()\n",
    "    X = df.drop(columns=[\"readmitted\"]).values\n",
    "\n",
    "    return X, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3c9a2",
   "metadata": {},
   "source": [
    "## Representative Subset Sampling\n",
    "We will take a representative sample subset (default: 1000) of the diabetic dataset as DBScan scales quadratically with sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096251ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, target = load_and_preprocess_data(\"data/diabetic_data.csv\")\n",
    "sample_size = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "if len(X) > sample_size:\n",
    "        print(f\"Sampling {sample_size} points for DBScan...\")\n",
    "        idx = np.random.choice(len(X), sample_size, replace = False)\n",
    "        X = X[idx]\n",
    "        target = target.iloc[idx].values\n",
    "else:\n",
    "    target = target.values\n",
    "\n",
    "X.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278804b",
   "metadata": {},
   "source": [
    "## PCA Dimensionality Reduction \n",
    "\n",
    "We reduce dimensionality to **50 principal components**, preserving ~85–90% variance.  We use PCA to reduce dimensionality of the dataset to **50 principal components**, preserving ~85-90% variance. Doing so allows us to speed up the clustering algorithm and avoid complications with higher dimension non-linearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 50 \n",
    "pca = PCA(n_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"PCA shape:\", X_pca.shape)\n",
    "print(\"PCA done running. Explained variance:\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d169b",
   "metadata": {},
   "source": [
    "##  Running DBScan Clustering\n",
    "\n",
    "As DBScan does not require cluster counts to run, we instead run DBScan on the sample dataset. The value of epsilon and the number of minimum points set in a neighbourhood determines how well-separated clusters are. Thus, we will compute the **silhouette coefficient* for different values of epsilon and minPts. \n",
    "\n",
    "*Sihouette score measures the well-separateness of clusters, with higher = better with a max of 1.0.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1879eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "best_score = -1\n",
    "best_clustering = None\n",
    "\n",
    "db = DBScan(0.2, 5) \n",
    "clustering = db.fit(X_pca)\n",
    "\n",
    "sil = silhouette_score(X_pca, clustering)\n",
    "silhouettes.append(sil)\n",
    "print(f\"Silhouette = {sil:.4f}\")\n",
    "\n",
    "if sil > best_score:\n",
    "    best_score = sil\n",
    "    best_clustering = clustering.copy()\n",
    "\n",
    "print(\"\\nBest clustering: \", best_clustering, \"with silhouette score: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d18105",
   "metadata": {},
   "source": [
    "##  Sihouette Coefficient Plot \n",
    "\n",
    "The plot below shows silhouette coefficients for different values of epsilon.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1be9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(silhouettes, \"o-\", color = \"steelblue\")\n",
    "plt.title(\"DBScan Clustering Silhouette Scores\")\n",
    "plt.xlabel(\"Epsilon Value\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483628b3",
   "metadata": {},
   "source": [
    "## 2D Visualization of Clusters \n",
    "\n",
    "For visualization, we plot the 50-dim PCA data into a 2D scatter plot. \n",
    "\n",
    "*Different colour = different cluster.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba05ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vis = PCA(n_components = 2).fit_transform(X)\n",
    "cmap = matplotlib.colormaps[\"tab20\"]\n",
    "unique_clusters = np.unique(clustering)\n",
    "num_clusters = len(unique_clusters)\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "for i, cid in enumerate(unique_clusters):\n",
    "    mask = clustering == cid\n",
    "    c_value = i / max(num_clusters - 1, 1) if num_clusters > 1 else 0\n",
    "    color = cmap(c_value)\n",
    "    plt.scatter(X_vis[mask, 0], X_vis[mask, 1],\n",
    "        s = 20, alpha = 0.6, color = color, edgecolors = \"black\",\n",
    "        linewidths = 0.5, label = f\"Cluster {cid}\")\n",
    "    \n",
    "plt.title(f\"DBScan Clustering\", fontsize = 14, fontweight = \"bold\")\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=12)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize=12)\n",
    "plt.legend(bbox_to_anchor = (1.05, 1), loc = \"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3af07",
   "metadata": {},
   "source": [
    "# **Interpretation & Discussion**\n",
    "## **Strengths of DBScan Clustering**\n",
    "\n",
    "* Does not require initialization of centroids and/or clusters\n",
    "* Robust to noise/outliers in dataset \n",
    "* Reveals correlation between features in the same cluster due to objects in cluster being density-reachable\n",
    "\n",
    "---\n",
    "\n",
    "## **Problems of DBScan Clustering**\n",
    "\n",
    "* Running time at worst case (choice of epsilon, minPts, noise/cluster separation) is **O(n2)** → requires representative subset of dataset\n",
    "* As with hierarchical clustering, results are dependent on euclidean distance of points after PCA dimensionality reduction. \n",
    "* Difficulty in separating noise from clusters and may negatively impact results\n",
    "* Varying density in the dataset may result in inaccurate clustering. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
